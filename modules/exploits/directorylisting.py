import requests
import urllib.parse
from bs4 import BeautifulSoup
import re
import time
import random

class DirectoryListingScanner:
    def __init__(self):
        self.options = {
            "URL": "",
            "TIMEOUT": "10",
            "DELAY": "0",
            "COOKIE": "",
            "USER_AGENT": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "HEADERS": "",
            "RECURSIVE": True,
            "MAX_DEPTH": "3",
            "FOLLOW_REDIRECTS": True,
            "VERIFY_SSL": False,
            "COMMON_PATHS": True,
            "CUSTOM_PATHS": "",
            "EXTENSIONS": "",
            "IGNORE_CODES": "404",
            "SAVE_RESPONSES": False
        }
        
        self.common_paths = [
            "admin", "administrator", "login", "wp-admin", "wp-login", "cpanel", "panel", "control", "admincp",
            "administer", "user", "users", "account", "accounts", "member", "members", "dashboard", "manage",
            "manager", "management", "config", "configuration", "setup", "install", "test", "testing", "dev",
            "development", "staging", "tmp", "temp", "backup", "backups", "old", "copy", "bak", "backup~",
            "conf", "cfg", "config.php", "config.inc", "config.ini", "config.xml", "config.json", "config.yml",
            "sql", "database", "db", "mysql", "sqlitedb", "data", "files", "file", "upload", "uploads",
            "download", "downloads", "images", "img", "image", "media", "video", "videos", "audio", "doc",
            "docs", "documents", "pdf", "phpinfo", "info", "info.php", "phpinfo.php", "status", "server-status",
            "index", "index.php", "index.html", "index.htm", "default", "default.php", "default.html", "default.htm",
            "home", "home.php", "home.html", "home.htm", "root", "api", "rest", "graphql", "xmlrpc", "soap",
            "wsdl", "rss", "feed", "atom", "sitemap", "sitemap.xml", "robots.txt", "humans.txt", ".htaccess",
            ".htpasswd", "web.config", "app.config", "machine.config", "application.config", "access.log",
            "error.log", "server.log", "logs", "log", "cgi-bin", "bin", "etc", "lib", "include", "includes",
            "inc", "class", "classes", "model", "models", "view", "views", "controller", "controllers",
            "template", "templates", "cache", "session", "sessions", "tmp", "temp", "backup", "backups",
            "old", "archive", "archives", "private", "protected", "secure", "secret", "internal", "hidden",
            "assets", "static", "content", "storage", "store", "shop", "cart", "checkout", "payment", "billing",
            "invoice", "invoices", "order", "orders", "customer", "customers", "client", "clients", "user",
            "users", "admin", "administrator", "auth", "authenticator", "security", "license", "licenses",
            "key", "keys", "token", "tokens", "password", "passwords", "secret", "secrets", "private", "protected",
            "internal", "hidden", "debug", "debugging", "trace", "tracing", "profile", "profiling", "monitor",
            "monitoring", "stats", "statistics", "analytics", "report", "reports", "log", "logs", "audit",
            "auditing", "history", "backup", "backups", "restore", "migration", "migrate", "update", "upgrade",
            "patch", "patches", "fix", "fixes", "bug", "bugs", "error", "errors", "exception", "exceptions",
            "warning", "warnings", "notice", "notices", "info", "information", "help", "support", "faq",
            "documentation", "docs", "readme", "changelog", "news", "blog", "forum", "forums", "community",
            "social", "facebook", "twitter", "instagram", "linkedin", "youtube", "vimeo", "flickr", "pinterest",
            "tiktok", "snapchat", "whatsapp", "telegram", "discord", "slack", "microsoft", "google", "amazon",
            "apple", "facebook", "twitter", "instagram", "linkedin", "youtube", "vimeo", "flickr", "pinterest",
            "tiktok", "snapchat", "whatsapp", "telegram", "discord", "slack"
        ]
        
        self.directory_indicators = [
            "Index of", "Directory Listing", "Directory of", "Parent Directory", "Name", "Last modified",
            "Size", "Description", "Owner", "Group", "Directory", "File", "[To Parent Directory]",
            "<title>Index of", "<h1>Index of", "<pre>", "<table", "<tr", "<td", "<a href=",
            "Apache/2", "nginx", "Server at", "Port"
        ]
        
        self.file_indicators = [
            ".php", ".html", ".htm", ".asp", ".aspx", ".jsp", ".jspx", ".do", ".action", ".cgi", ".pl",
            ".py", ".rb", ".php3", ".php4", ".php5", ".phtml", ".shtml", ".cfm", ".cfml", ".yaws",
            ".xml", ".json", ".txt", ".log", ".conf", ".ini", ".bak", ".backup", ".old", ".orig",
            ".save", ".tmp", ".temp", ".swp", ".swo", ".cache", ".lock", ".pid", ".db", ".sqlite",
            ".sql", ".mdb", ".accdb", ".dbf", ".csv", ".tsv", ".xls", ".xlsx", ".doc", ".docx",
            ".pdf", ".zip", ".tar", ".gz", ".bz2", ".7z", ".rar", ".deb", ".rpm", ".dmg", ".iso",
            ".exe", ".msi", ".dll", ".so", ".dylib", ".a", ".lib", ".o", ".obj", ".class", ".jar",
            ".war", ".ear", ".apk", ".ipa", ".app", ".bin", ".hex", ".dat", ".data", ".config",
            ".properties", ".yml", ".yaml", ".toml", ".pem", ".crt", ".key", ".p12", ".pfx", ".der",
            ".csr", ".jks", ".keystore", ".truststore", ".license", ".lic", ".reg", ".inf", ".sys",
            ".drv", ".bat", ".cmd", ".sh", ".ps1", ".vbs", ".js", ".css", ".scss", ".less", ".sass",
            ".coffee", ".ts", ".jsx", ".tsx", ".vue", ".html", ".htm", ".xhtml", ".xml", ".svg",
            ".png", ".jpg", ".jpeg", ".gif", ".bmp", ".tiff", ".tif", ".webp", ".ico", ".cur",
            ".mp3", ".mp4", ".m4a", ".wav", "flac", ".ogg", ".wma", ".avi", ".mov", ".wmv", ".flv",
            ".swf", ".mkv", ".webm", ".m4v", ".3gp", ".asf", ".rm", ".rmvb", ".vob", ".ts", ".mts",
            ".m2ts", ".f4v", ".f4p", ".f4a", ".f4b"
        ]
        
        self.responses = {}
    
    def set_option(self, key, value):
        self.options[key] = value
    
    def get_option(self, key):
        return self.options[key]
    
    def run(self):
        url = self.get_option("URL")
        timeout = int(self.get_option("TIMEOUT"))
        delay = int(self.get_option("DELAY"))
        recursive = self.get_option("RECURSIVE")
        max_depth = int(self.get_option("MAX_DEPTH"))
        follow_redirects = self.get_option("FOLLOW_REDIRECTS")
        verify_ssl = self.get_option("VERIFY_SSL")
        common_paths = self.get_option("COMMON_PATHS")
        custom_paths = self.get_option("CUSTOM_PATHS")
        extensions = self.get_option("EXTENSIONS")
        ignore_codes = self.get_option("IGNORE_CODES").split(",")
        save_responses = self.get_option("SAVE_RESPONSES")
        
        headers = {
            "User-Agent": self.get_option("USER_AGENT"),
            "Cookie": self.get_option("COOKIE")
        }
        
        if self.get_option("HEADERS"):
            try:
                for header in self.get_option("HEADERS").split("\\n"):
                    if ":" in header:
                        key, value = header.split(":", 1)
                        headers[key.strip()] = value.strip()
            except:
                pass
        
        paths_to_test = []
        
        if common_paths:
            paths_to_test.extend(self.common_paths)
        
        if custom_paths:
            paths_to_test.extend(custom_paths.split(","))
        
        if extensions:
            ext_list = extensions.split(",")
            for path in paths_to_test[:]:
                for ext in ext_list:
                    paths_to_test.append(f"{path}.{ext}")
        
        directories = []
        
        for path in paths_to_test:
            if delay > 0:
                time.sleep(delay)
            
            test_url = self.normalize_url(url, path)
            is_directory, content = self.check_directory_listing(test_url, timeout, headers, follow_redirects, verify_ssl, ignore_codes)
            
            if is_directory:
                directories.append({
                    "url": test_url,
                    "path": path,
                    "type": "directory"
                })
                
                if save_responses:
                    self.responses[test_url] = content
                
                if recursive and max_depth > 1:
                    sub_directories = self.scan_recursive(test_url, timeout, headers, follow_redirects, verify_ssl, ignore_codes, delay, max_depth - 1)
                    directories.extend(sub_directories)
        
        if directories:
            return {
                "success": True,
                "message": f"Found {len(directories)} directories with listing enabled",
                "directories": directories,
                "responses": self.responses if save_responses else None
            }
        else:
            return {
                "success": True,
                "message": "No directories with listing enabled found"
            }
    
    def normalize_url(self, base_url, path):
        if not base_url.endswith("/"):
            base_url += "/"
        
        if path.startswith("/"):
            path = path[1:]
        
        return urllib.parse.urljoin(base_url, path)
    
    def check_directory_listing(self, url, timeout, headers, follow_redirects, verify_ssl, ignore_codes):
        try:
            response = requests.get(
                url,
                timeout=timeout,
                headers=headers,
                allow_redirects=follow_redirects,
                verify=verify_ssl
            )
            
            if str(response.status_code) in ignore_codes:
                return False, None
            
            content = response.text.lower()
            
            for indicator in self.directory_indicators:
                if indicator.lower() in content:
                    return True, response.text
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            links = soup.find_all('a')
            if len(links) > 2:
                parent_found = False
                file_links = 0
                
                for link in links:
                    href = link.get('href', '')
                    text = link.get_text(strip=True).lower()
                    
                    if href == "../" or text == "parent directory":
                        parent_found = True
                    
                    if any(href.endswith(ext) for ext in [".php", ".html", ".htm", ".asp", ".aspx", ".jsp", ".py", ".pl", ".cgi"]):
                        file_links += 1
                
                if parent_found and file_links > 0:
                    return True, response.text
            
            return False, None
            
        except Exception:
            return False, None
    
    def scan_recursive(self, base_url, timeout, headers, follow_redirects, verify_ssl, ignore_codes, delay, max_depth):
        if max_depth <= 0:
            return []
        
        sub_directories = []
        
        try:
            response = requests.get(
                base_url,
                timeout=timeout,
                headers=headers,
                allow_redirects=follow_redirects,
                verify=verify_ssl
            )
            
            soup = BeautifulSoup(response.text, 'html.parser')
            links = soup.find_all('a')
            
            for link in links:
                href = link.get('href', '')
                
                if href and not href.startswith('http') and not href.startswith('?') and not href.startswith('#') and href != '../':
                    if delay > 0:
                        time.sleep(delay)
                    
                    full_url = self.normalize_url(base_url, href)
                    is_directory, content = self.check_directory_listing(full_url, timeout, headers, follow_redirects, verify_ssl, ignore_codes)
                    
                    if is_directory:
                        sub_directories.append({
                            "url": full_url,
                            "path": href,
                            "type": "directory"
                        })
                        
                        if self.get_option("SAVE_RESPONSES"):
                            self.responses[full_url] = content
                        
                        if max_depth > 1:
                            sub_sub_directories = self.scan_recursive(full_url, timeout, headers, follow_redirects, verify_ssl, ignore_codes, delay, max_depth - 1)
                            sub_directories.extend(sub_sub_directories)
        
        except Exception:
            pass
        
        return sub_directories
